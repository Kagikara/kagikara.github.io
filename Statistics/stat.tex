\documentclass[lang=cn,10pt]{elegantbook}

\title{统计学基础}


\extrainfo{咕咕嘎嘎}

\setcounter{tocdepth}{3}


\usepackage{pdfpages}
\usepackage{array}
\usepackage{tikz-cd}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{braket}
\usepackage{faktor}
\addbibresource{refs.bib} % 指定 .bib 文件（含扩展名）
% 修改标题页的橙色带
\definecolor{customcolor}{RGB}{30, 60, 114}
\colorlet{coverlinecolor}{customcolor}
\cover{steve-busch-Zm0MOhF7iHE-unsplash (1).pdf}
\newcommand{\ccr}[1]{\makecell{{\color{#1}\rule{1cm}{1cm}}}}

\begin{document}
	
	\definecolor{structurecolor}{RGB}{18, 33, 79}
	\definecolor{main}{RGB}{110, 170, 235}
	\definecolor{second}{RGB}{46, 139, 139}
	\definecolor{third}{RGB}{88, 63, 125}
	
	\maketitle
	\frontmatter
	
	\tableofcontents
	
	\mainmatter
    \chapter{数理统计基础}

    \chapter{线性模型}
    \begin{introduction}
    \item 一元线性模型
    \item 多元线性模型
    \end{introduction}
    我们希望从数据上找到两个现象之间的关系，比如说物理学中我们寻找质量不变时加速度和力的关系到底是怎样的？我们应该用什么函数模型来描述现象之间的关系？为了解决这些问题，我们把得到的数据画到坐标纸上，试图用一条线来经过尽可能多的点——我们称这种行为为“拟合”。这里就蕴含了线性回归的思想：尝试用一个“回归方程”来靠近尽可能多的数据点，并使这个方程与所有点的“距离”尽可能地小，从而得到一个数学式的解释。我们在线性模型中就要解决这种问题：我们要建立什么样的模型，在什么样的标准下才能最小化“距离”之和？这个模型和它的参数又有什么性质？这个模型真实可靠、有现实意义吗？下面我们从一元线性模型开始阐述这个课题。
    \section{一元线性模型}
    一元回归分析研究一个变量（因变量\(y\)）对一个解释变量（自变量\(x\)）的依从关系。
    \subsection{回归直线}

    我们假设，在建模合理的情况下（变量种类、数量选择合理，它们的关系也处理得当）数据在回归直线旁两侧随机波动。也就是说：如果我们设回归直线为\(f(x)=E[Y|X]\)的话，那么
    \[y=E[Y|X]+\mu.\]
    这里的条件期望我们称为回归函数，它是在已知\(x\)时对\(y\)在均方误差意义下最好的估计量。关于条件期望的详细内容，见An\&P条件期望部分内容。需要注意的是，这里的\(x\)虽然被称为自变量，但是我们把它看作固定的常数。事实上，它们就是我们收集到的数据，自然是已知的。而这里随机性的来源是\(\mu\),它是一个随机变量，因此\(y\)也是一个随机变量。\(\mu\)就是数据在回归直线两侧的波动，我们称其为离差，或随机误差项。它是无法观测的。离差代表了样本点和回归方程的距离。

    在线性回归中，我们取回归函数\(E[Y|X]\)为线性函数——这里的“线性”是相对于系数\(\beta_i\)而非自变量\(x_i\)而言的。一元线性回归中，我们取回归函数
    \[E[Y|X]=\beta_0+\beta_1x.\]
    于是模型就变为
    \[y_i=\beta_0+\beta_1x_i+\mu_i.\]
    我们称这个式子为总体回归函数，我们在散点图中画的拟合直线并不是总体回归函数，因为总体回归函数总是未知的。注意，这里的\(\beta_0,\beta_1\)都是未知但是固定的数，我们要估计这两个参数。

    所谓的“拟合”就是通过估计模型的参数试图预测两个现象之间的数量关系。我们手上拿着两种数据：自变量\(x_i\)和因变量\(y_i\),现在就要通过估计的模型算出因变量的估计\(\hat{y_i}\),这要通过估计模型中的两个参数\(\beta_0,\beta_1\)来实现，这时
    \[\hat{y_i}=\hat{\beta_0}+\hat{\beta_1}x_i.\]
    这就是样本回归函数。我们用样本回归函数估计总体回归函数。

    在试图用样本回归函数估计了总体回归函数后，我们当然不能期望两者完全一样。也就是存在残差\(\hat{\mu_i}\):
    \[y_i=\hat{y_i}+\hat{\mu_i}.\]
    以上就是用样本回归函数估计总体回归函数的想法。我们现在更细致一些来讨论线性回归模型的假设和性质。
    \subsection{最小二乘估计}

    经典线性回归模型(CLRM)对随机项\(\mu\)做了以下假设,这些假设使最小二乘估计得出的参数估计具有优良性质：
    \begin{enumerate}
        \item 条件均值为0：\[E[\mu_i|X_i]=0.\]
        \item 同方差：\[V[\mu_i|X_i]=\sigma_\mu^2.\]
        \item 无序列相关：\[cov(\mu_i,\mu_j|X_i,X_j)=0,i\ne j.\]
        \item 正态假设：\[\mu_i\sim N(0,\sigma_\mu^2).\]
    \end{enumerate}

    如何估计模型中的参数\(\beta_0,\beta_1\)?我们想画出来的拟合线当然是越靠近数据点越好。但是怎么定义“近”这个概念？自然联想到距离。但是欧氏距离在计算上不好处理，因此用估计直线和样本点之间的距离平方和，也就是残差平方和
    \[Q=\sum_{i=1}^{n}\hat{\mu_i}^2=\sum_{i=1}^{n}(y_i-\hat{\beta_0}-\hat{\beta_1}x_i)^2\]
    来衡量。我们要最小化\(Q\)，这只要求导求出极值点即可。
    \begin{proof}
        分别对\({\beta_0},{\beta_1}\)求偏导：
        \begin{align*}
        \left.\frac{\partial Q}{\partial\beta_0}\right|_{\beta_0=\hat{\beta}_0}
        &= -2\sum_{i=1}^{n}\bigl(y_i-\hat{\beta}_0-\hat{\beta}_1 x_i\bigr) = 0,\\
        \left.\frac{\partial Q}{\partial\beta_1}\right|_{\beta_1=\hat{\beta}_1}
        &= -2\sum_{i=1}^{n}\bigl(y_i-\hat{\beta}_0-\hat{\beta}_1 x_i\bigr)x_i = 0.
        \end{align*}
        整理上面的方程：
        \begin{align*}
            \hat{\beta}_0+\hat{\beta}_1\overline{x}&=\overline{y},\\
            \hat{\beta}_0(\sum_{i=1}^{n}x_i)+\hat{\beta}_1(\sum_{i=1}^{n}x_i^2)&=\sum_{i=1}^{n}x_iy_i.
        \end{align*}
        于是得到
        \begin{align*}
            \hat{\beta}_0&=\overline{y}-\hat{\beta}_1\overline{x},\\
            \hat{\beta}_1&=\frac{L_{xy}}{L_{xx}}.
        \end{align*}
        其中
        \[L_{xy}=\sum_{i=1}^{n}(x_i-\overline{x})(y_i-\overline{y})=\sum_{i=1}^{n}x_iy_i-n\overline{x}\overline{y},\]
        \[L_{xx}=\sum_{i=1}^{n}(x_i-\overline{x})^2=\sum_{i=1}^{n}x_i^2-n\overline{x}^2.\]
    \end{proof}
    \begin{note}
    实际上求偏导直接得出的方程组说明了残差的两条性质：
    \begin{enumerate}
        \item \(\sum_{i=1}^{n}\mu_i=0\);
        \item \(\sum_{i=1}^{n}\mu_ix_i=0\).
    \end{enumerate}
    \end{note}

    \subsection{最小二乘估计的性质}
    在经典线性回归模型的假设下，最小二乘估计具有很好的性质。
    \begin{property}
        \begin{enumerate}
            \item 线性性：\(\hat{\beta_0},\hat{\beta_1}\)都是\(y_i\)的线性函数，于是\(\hat{\beta_0},\hat{\beta_1}\)都服从正态分布；
            \item 无偏性：\(E[\hat{\beta_0}]=\beta_0,E[\hat{\beta_1}]=\beta_1\);
            \item 有效性(Gauss-Markov)：它是所有线性无偏估计量中具有最小方差的估计；
        \end{enumerate}
    \end{property}
    \begin{proof}
        我们证前两条，并顺带得出两个参数的方差，进而得到两个参数的分布。
        \begin{enumerate}
            \item 首先，可以证明\(\hat{\beta_1}\)可以改写成\[\frac{\sum_{i=1}^n(x_i-\overline{x})y_i}{\sum_{j=1}^{n}(x_j-\overline{x})^2},\]这就把\(\hat{\beta_1}\)改写成了\(y_i\)的线性组合。同理我们能把\(\hat{\beta_0}\)写成\[\frac{\sum_{i=1}^n[\frac{L_{xx}}{n}-\overline{x}(x_i-\overline{x})]y_i}{L_{xx}}.\]
            \item 现在证明无偏性。我们知道\(y_i\)也服从正态分布\(N(\beta_0+\beta_1x_i,\sigma^2)\),于是由期望线性性就知道\[E[\hat{\beta_1}]=\frac{\sum_{i=1}^n(x_i-\overline{x})}{\sum_{j=1}^{n}(x_j-\overline{x})^2}E[y_i]=\frac{\sum_{i=1}^n(x_i-\overline{x})}{\sum_{j=1}^{n}(x_j-\overline{x})^2}(\beta_0+\beta_1x_i)=\beta_1.\]其中用了\(\sum_{i=1}^n(x_i-\overline{x})=0,\sum_{i=1}^n(x_i-\overline{x})x_i=\sum_{i=1}^n(x_i-\overline{x})^2.\)
            
            于是\(\beta_0\)就有：\[E[\beta_0]=E[\overline{y}-\hat{\beta}_1\overline{x}]=\beta_0+\beta_1x_i-\beta_1 x_i=\beta_0.\]
            
            下面我们来看两个参数的方差。由于\(\mu_i\)互相独立，因此也有\(y_i\)互相独立。于是\[V[\hat{\beta_1}]=\sum_{i=1}^{n}(\frac{x_i-\overline{x}}{L_{xx}})^2V[y_i]=\frac{\sigma^2}{L_{xx}}.\]
            因此\(\hat{\beta_1}\sim N(\beta_1,\frac{\sigma^2}{L_{xx}})\).

            另外，\[V[\hat{\beta_0}]=\sigma^2\sum_{i=1}^{n}(\frac{1}{n}-\frac{\overline{x}(x_i-\overline{x})}{L_{xx}})^2=(\frac{1}{n}+\frac{\overline{x}^2}{L_{xx}})\sigma^2.\]
            因此\(\hat{\beta_0}\sim N(\beta_0,(\frac{1}{n}+\frac{\overline{x}^2}{L_{xx}})\sigma^2)\).
        \end{enumerate}
    \end{proof}
    \begin{note}
        我们从上面得出的参数分布可知，对于不同于\(x_i,i=1,2,\cdots n\)的样本点\(x_0\)来说，它的预测值
        \[\hat{y}_0=\hat{\beta}_0+\hat{\beta}_1x_0=\overline{y}-\hat{\beta}_1\overline{x}+\hat{\beta}_1x_0=\frac{\sum_{i=1}^{n}((x_i-\overline{x})(x_0-\overline{x})+\frac{L_{xx}}{n})y_i}{L_{xx}},\]
        同样服从正态分布，即
        \[\hat{y}_0\sim N(\beta_0+\beta_1x_0,(\frac{1}{n}+\frac{(x_0-\overline{x})^2}{L_{xx}})\sigma^2).\]
    \end{note}

        我们得出的样本回归线有几条性质：
    \begin{enumerate}
        \item 过点\((\overline{x},\overline{y})\);
        \item \(E[\hat{y_i}]=E[y_i]\);
        \item 残差估计值和\(\hat{y_i},x_i\)都不相关。
    \end{enumerate}

    \subsection{极大似然估计}
    从极大似然估计的角度，我们也可以得出\(\hat{\beta}_0,\hat{\beta}_1\)，还有\(\sigma^2\)的估计。首先写出似然函数,由\(y_i\sim N(\beta_0+\beta_1 x_i,\sigma^2)\)：
    \[f(y;x_1,x_2,\cdots x_n)=\frac{1}{(\sqrt{2\pi}\sigma)^n}\exp{(-\frac{\sum_{i=1}^{n}(y-\beta_0-\beta_1 x_i)^2}{2\sigma^2})},\]
    取对数：
    \[\log{f}=-\frac{n}{2}\log{(2\pi\sigma^2)}-\frac{\sum_{i=1}^{n}(y-\beta_0-\beta_1 x_i)^2}{2\sigma^2},\]
    对于\(\hat{\beta}_0,\hat{\beta}_1\)，求最值的步骤和最小二乘法相同。下面我们看\(\sigma^2\)的极大似然估计。对\(\sigma^2\)求偏导并令其为零：
    \[\frac{\partial}{\partial \sigma^2}\log{f}=-\frac{n}{2\sigma^2}+\frac{\sum_{i=1}^{n}(y-\beta_0-\beta_1 x_i)^2}{2(\sigma^2)^2}=0,\]
    解得
    \[\hat{\sigma}^2=\frac{1}{n}\sum_{i=1}^{n}\hat{\mu_i}^2.\]

    然而\(\hat{\sigma}^2\)并不是\(\sigma^2\)的无偏估计。我们经常使用的是无偏估计版本:
    \[\hat{\sigma}^2=\frac{1}{n-2}\sum_{i=1}^{n}\hat{\mu_i}^2.\]

    
    \subsection{一元线性回归模型的检验}
    在拟合之后得到的模型在描述现象之间的关系时是否合适？例如，如果我们研究收入和消费时，发现收入越多消费反而越少，这时我们就会不禁怀疑做出的模型是否可靠。实际上我们需要一种客观的方法来检验模型是否有实际意义，也就是说检验自变量是否真的对因变量有影响？哪些自变量对因变量有影响？这样的影响是合乎道理的吗？下面我们就根据假设检验原理导出检验模型有效性的工具。

    统计检验分为模型参数显著性检验（t检验）、模型显著性检验（F检验），拟合优度检验、相关系数检验。

    先看t检验。t检验的对象是模型的单个参数是否显著，也就是对于总体回归函数
    \[y_i=\sum_{j=0}^{n}\beta_jx_{ij}+\mu_i,\ x_{i0}=1,\]
    \begin{enumerate}
        \item 原假设\(H_0:\beta_j=0\);
        \item 对立假设\(H_1:\beta_j\ne0\).
    \end{enumerate}
    在一元回归模型中，我们只考虑\(j=1\)的情况。
    我们知道\(\hat{\beta}_1\sim N(\beta_1,\frac{\sigma^2}{L_{xx}})\)，所以当\(H_0\)成立时，\(\hat{\beta}_1\sim N(0,\frac{\sigma^2}{L_{xx}})\)，由于方差未知，因此可以构造检验统计量
    \[t=\frac{\hat{\beta}_1/\sqrt{(\frac{\sigma^2}{L_{xx}})}}{\sqrt{\frac{\hat{\sigma}^2}{\sigma^2}}}=\frac{\hat{\beta}_1\sqrt{L_{xx}}}{\hat{\sigma}}\sim t(n-2),\]
    其中\(\hat{\sigma}^2\)的定义取\(\sigma^2\)的无偏估计.所以上述检验的拒绝域为(水平为\(\alpha\))
    \[W=\{t:|t|>t_{\alpha/2}(n-2)\}.\]
    \begin{note}
        正态总体\(N(\mu,\sigma^2)\)中抽取的样本\(x_i,i=1,2,\cdots n\)满足
        \[\frac{\sum_{i=1}^{n}(x_i-\overline{x})^2}{\sigma^2}\sim \chi^2_{n-1},\]
        而残差平方和由于在解最小二乘估计方程组时添加了两个条件，因此自由度为\(n-2\),也就是
        \[\frac{\sum_{i=1}^{n}(y_i-\hat{y}_i)^2}{\sigma^2}\sim \chi^2_{n-2}.\]
        我们记残差平方和为\(SSE\)，则上式变为
        \[\frac{SSE}{\sigma^2}\sim \chi^2_{n-2}.\]
    \end{note}

    然后是F检验。F检验又称模型显著性检验，它检验的对象是整个模型中的自变量是否至少有一个对因变量有关。所以一元线性回归中的t检验和F检验实际上是等价的。
    \begin{enumerate}
        \item 原假设\(H_0:\beta_j=0,j=1,2,\cdots n\);
        \item 对立假设\(H_1:^\neg H_0\).
    \end{enumerate}
    我们由平方和分解：
    \[\sum_{i=1}^{n}(y_i-\overline{y})^2=\sum_{i=1}^{n}(y_i-\hat{y}_i)^2+\sum_{i=1}^{n}(\hat{y}_i-\overline{y})^2,\]
    简记为
    \[SST=SSE+SSR\ (S_T=S_E+S_R).\]
    其中\(SST\)是总离差平方和，它代表了实际数据和回归直线之间的距离，自由度为\(n-1\)；\(SSR\)是回归平方和，代表了拟合出的模型解释数据波动的能力，也就是预料之内的波动程度，自由度为\(1\)；\(SSE\)是残差平方和，它代表了数据拟合的优良程度，也代表着模型没能解释的数据波动情况，即意料之外的波动程度，自由度为\(n-2\)。于是我们想看去除自由度的影响后，衡量这个模型能解释多少数据波动情况，就得出了统计量\(F\):
    \[F=\frac{SSR/1}{SSE/(n-2)}.\]
    它代表了回归直线能解释的数据波动比例。

    实际上，\(\frac{SSR}{\sigma^2}\sim \chi^2_1,\frac{SSE}{\sigma^2}\sim \chi^2_{n-2}\)，因此
    \[F=\frac{SSR/1}{SSE/(n-2)}\sim F_{1,n-2}.\]
    检验的拒绝域为(水平为\(\alpha\))
    \[W=\{F:F>F_{\alpha}(1,n-2)\}.\]

    第三个检验是相关系数的显著性检验，这并不是一个假设检验，只是算出一个系数来判断两个因素之间是否有相关性、相关性有多强。

    我们称
    \[r=\frac{\sum_{i=1}^{n}(x_i-\overline{x})(y_i-\overline{y})}{\sqrt{\sum_{i=1}^{n}(x_i-\overline{x})^2}\sqrt{\sum_{i=1}^{n}(y_i-\overline{y})^2}}=\frac{L_{xy}}{\sqrt{L_{xx}}\sqrt{L_{yy}}}\]
    为简单相关系数，简称相关系数。它衡量了因变量、自变量之间的线性相关程度。由Cauchy-Schwartz：\(|r|\le1.\)

    由于\[r=\frac{L_{xy}}{\sqrt{L_{xx}}\sqrt{L_{yy}}}=\hat{\beta}_1\sqrt{\frac{L_{yy}}{L_{xx}}},\]因此所谓线性关系显著就代表了\(\hat{\beta}_1\)是否显著。

    下面给出三种检验的关系：
    \begin{enumerate}
        \item t检验和F检验：我们知道，自由度为\(1,n\)的\(F\)分布随机变量可以写成\[F_{1,n}=\frac{\chi^2_1/1}{\chi^2_n/n},\]而\(\chi^2_1=(N(0,1))^2\)，因此还有\[F_{1,n}=\frac{(N(0,1))^2}{\chi^2_n/n},\]而自由度为\(n\)的\(t\)分布的构成为\(\frac{N(0,1)}{\sqrt{\chi^2_n/n}}\),因此\[F_{1,n}=t_n^2.\]从而我们知道t检验和F检验在一元回归中是等价的。
        \item 相关系数检验和F检验：实际上\(SST=L_{yy},SSR=\hat{\beta}_1^2L_{xx}.\)因此对于F检验，我们有\[\frac{1}{1+\frac{n-2}{F}}=\frac{SSR}{SST}=\hat{\beta}_1^2\frac{L_{xx}}{L_{yy}}=r^2.\]
    \end{enumerate}

    最后一个检验是拟合优度检验，它比较了不同自变量对因变量的解释程度，也就是解决“哪个指标能更好的解释因变量”这个问题。我们引进“解释程度”变量：
    \[R^2=\frac{SSR}{SST}=(r)^2.\]
    这个比例代表了样本偏差中回归偏差的占比，越靠近1说明拟合效果越好。

    \subsection{一元线性回归模型参数的区间估计}
     假设检验和置信区间都应用了枢轴量来构造，因此具有天然的一一对应关系。我们考察\(\beta_0,\beta_1\)的置信区间。

     我们知道\(\hat{\beta}_1\sim N(\beta_1,\frac{\sigma^2}{L_{xx}})\)，由于方差未知，我们构造t置信区间,枢轴量为
     \[\frac{\hat{\beta}_1-\beta_1}{\sqrt{\frac{\hat{\sigma}^2}{L_{xx}}}}\sim t_{n-2},\]
     置信区间如下构造：
     \[P(|\frac{\hat{\beta}_1-\beta_1}{\sqrt{\frac{\hat{\sigma}^2}{L_{xx}}}}|\le t_{\alpha/2}(n-2))=1-\alpha,\]
     解得\(1-\alpha\)置信区间为
     \[\hat{\beta}_1\pm \frac{t_{\alpha/2}(n-2)\hat{\sigma}}{\sqrt{L_{xx}}}.\]
    对于\(\hat{\beta_0}\sim N(\beta_0,(\frac{1}{n}+\frac{\overline{x}^2}{L_{xx}})\sigma^2)\)也可同理构造t统计量。

    \subsection{预测与控制}
    单值控制：
    \begin{enumerate}
        \item \(\hat{y}_0=\hat{\beta}_0+\hat{\beta}_1x_0;\)
        \item \(E[\hat{y}_0]=E[y_0]=E[Y|X=x_0].\)
    \end{enumerate}
    这两条分别代表了对于个体的预测和对于群体均值的预测。

    区间预测：
    我们已经说明了预测值\(\hat{y}_0\)和实际值\(y_0\)的分布为
    \[\hat{y}_0\sim N(\beta_0+\beta_1x_0,(\frac{1}{n}+\frac{(x_0-\overline{x})^2}{L_{xx}})\sigma^2),\ y_0\sim N(\beta_0+\beta_1x_0,\sigma^2),\]
    并且\(\hat{y}_0,y_0\)相互独立：因为\(\hat{y}_0\)是\(y_i,i=1,2,\cdots n\)的线性函数，而\(y_0\)和其他数据都无关，因此\(y_0-\hat{y}_0\)同样服从正态分布
    \[N(0,(1+\frac{1}{n}+\frac{(x_0-\overline{x})^2}{L_{xx}})\sigma^2).\]
    这样我们又可以得到\(y_0\)水平为\(1-\alpha\)的t置信区间：
    \[\hat{y}_0\pm t_{\alpha/2}(n-2)\sqrt{(1+\frac{1}{n}+\frac{(x_0-\overline{x})^2}{L_{xx}})\hat{\sigma}^2}=\hat{y}_0\pm t_{\alpha/2}(n-2)\sqrt{1+h_{00}}\hat{\sigma}.\]
    其中\(h_{00}=\frac{1}{n}+\frac{(x_0-\overline{x})^2}{L_{xx}}.\)若选择\(\alpha=5\%\),则置信区间约为\(\hat{y}\pm 2\hat{\sigma}\).

    同理可得\(\hat{y}_0-E[y_0]\)的分布为
    \[N(0,(\frac{1}{n}+\frac{(x_0-\overline{x})^2}{L_{xx}})\sigma^2),\]
    得到\(E[y_0]\)水平为\(1-\alpha\)的t置信区间：
    \[\hat{y}_0\pm t_{\alpha/2}(n-2)\sqrt{h_{00}}\hat{\sigma}.\]
    \begin{note}
        对比得到，均值预测的置信区间半径小于单值预测置信区间，因为总体更稳定。
    \end{note}
    \section{多元线性回归模型}
    \subsection{模型形式}
    有多个解释变量的线性回归模型就是多元线性回归模型。我们可以类比一元回归模型的思想来看待它。有\(p\)个解释变量的\(p\)元总体回归函数形式为
    \[y = \beta_0 + \beta_1x_1+\beta_2x_2+\cdots+\beta_px_p+\epsilon.\]
    其中\(\beta_i,i=0,1,2,\cdots p\)是\(p+1\)个未知常数；\(x_i,i=1,2,\cdots p\)是因变量，它们是确定的数据；随机误差\(\epsilon\)是随机变量，它是数据的随机波动造成的误差，或者说是和回归函数之间的偏移；\(y\)是因变量，由于\(\epsilon\)的影响也是一个随机变量。

    假设观察到了\(n\)组数据\(y_i=\beta_0 + \beta_1x_{i1}+\beta_2x_{i2}+\cdots+\beta_px_{ip}+\epsilon_i,i=1,2,\cdots n\),我们用矩阵的形式来记录这多组因变量、自变量、回归系数与随机误差：

    \[\mathbf{y} =
    \begin{bmatrix}
        y_1\\
        y_2\\
        \vdots\\
        y_n
    \end{bmatrix}
    ,
    \mathbf{X} =
    \begin{bmatrix}
        1 & x_{11} & x_{12} & \cdots & x_{1p} \\
        1 & x_{21} & x_{22} & \cdots & x_{2p} \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        1 & x_{n1} & x_{n2} & \cdots & x_{np}
    \end{bmatrix}
    ,
    \mathbf{\beta} =
    \begin{bmatrix}
        \beta_0\\
        \beta_1\\
        \vdots\\
        \beta_p\\
    \end{bmatrix}
    ,
    \mathbf{\epsilon} =
    \begin{bmatrix}
        \epsilon_1\\
        \epsilon_n\\
        \vdots\\
        \epsilon_n\\
    \end{bmatrix}
    \]
    于是上面的\(n\)个线性组合就能被紧凑地写成矩阵形式
    \[\mathbf{y}=\mathbf{X}\mathbf{\beta}+\mathbf{\epsilon}.\]
    \begin{note}
        注意这里的\(\mathbf{X}\)比原数据多加了一列1向量，这是因为要配合\(\beta_0\)的乘法。这样\(\mathbf{X}\)就是\(n\times p+1\)的矩阵。
    \end{note}
    和一元时的情况同理，我们也对随机误差项做一些假设。
    \begin{enumerate}
        \item \(\mathbf{X}\)列满秩。这就要求列向量之间线性无关，排除了自变量多重共线性的可能。同时满足\(rank(\mathbf{X})=p+1<n\)，这意味着自变量个数不能超过数据个数，实际上这保证了后面OLS的解的存在性和稳定性。
        \item 随机误差项满足Gauss-Markov条件，也就是\[\begin{cases}
            E[\epsilon_i]=0,i=1,2,\cdots n\\
            cov(\epsilon_i,\epsilon_j)=\begin{cases}
                \sigma^2,i=j\\
                0,o.w.
            \end{cases}
            i,j=1,2,\cdots n.
        \end{cases}\]
        \item 随机误差项\(\epsilon_i,i=1,2,\cdots n\)独立同分布于正态分布\(N(0,\sigma^2)\).
    \end{enumerate}
    \begin{note}
        \\
        若满足Gauss-Markov条件，则\[E[\mathbf{y}]=E[\mathbf{X\beta+\epsilon}]=\mathbf{X\beta},\]\[cov(\mathbf{y,y})=cov(\mathbf{X\beta+\epsilon,X\beta+\epsilon})=cov(\epsilon,\epsilon)=\sigma^2\mathbf{I_{n}}.\]
    \end{note}
    \subsection{多元线性回归的回归系数最小二乘法}
    如何估计回归系数？我们自然可以和一元OLS的情形一样，对损失函数
    \[Q(\mathbf{\beta})=||\mathbf{y}-\mathbf{\beta}^T\mathbf{X}||_2^2\]
    求偏导并令偏导为0求出驻点解出\(\mathbf{\beta}\),但是我们可以用矩阵求导的办法来更紧凑地求出OLS的解。在这之前先介绍矩阵求导的规则。
    \begin{proposition}[矩阵求导]
        设有向量\(\mathbf{c},\mathbf{y}\)和矩阵\(\mathbf{X}\):
    \[\mathbf{c} =
    \begin{bmatrix}
        c_1\\
        c_2\\
        \vdots\\
        c_n
    \end{bmatrix}
    ,
        \mathbf{y} =
    \begin{bmatrix}
        y_1\\
        y_2\\
        \vdots\\
        y_n
    \end{bmatrix}
    ,
    \mathbf{X} =
    \begin{bmatrix}
        x_{11} & x_{12} & \cdots & x_{1p} \\
        x_{21} & x_{22} & \cdots & x_{2p} \\
        \vdots & \vdots & \ddots & \vdots \\
        x_{n1} & x_{n2} & \cdots & x_{np}
    \end{bmatrix}
    \]
    则：
    \begin{enumerate}
        \item (线性函数)\(\frac{\partial}{\partial \mathbf{y}}\mathbf{c^T\mathbf{y}}=\frac{\partial}{\partial \mathbf{y}}\mathbf{\mathbf{y}^T\mathbf{c}}=\mathbf{c}\);
        \item (二次型)\(\frac{\partial}{\partial \mathbf{y}}\mathbf{y}^T\mathbf{X}\mathbf{y}=(\mathbf{X}+\mathbf{X}^T)\mathbf{y}\);
        \item (矩阵乘法)\(\frac{\partial}{\partial \mathbf{y}}\mathbf{X}\mathbf{y}=\mathbf{X}^T\).
    \end{enumerate}
    \end{proposition}
    于是多元线性回归的OLS可以这样表述：
    \begin{theorem}[多元线性回归OLS]
        在假设设计矩阵列满秩的假设下，多元线性回归的回归系数最小二乘估计为
        \[\hat{\mathbf{\beta}}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}.\]
    \end{theorem}
    \begin{proof}
        损失函数
        \[Q(\mathbf{\beta})=||\mathbf{y}-\mathbf{\beta}^T\mathbf{X}||_2^2=(\mathbf{y}-\mathbf{\beta}^T\mathbf{X})^T\mathbf{y}-\mathbf{\beta}^T\mathbf{X}=\mathbf{y}^T\mathbf{y}-2\mathbf{\beta}^T\mathbf{X}^T\mathbf{y}+\mathbf{\beta}^T\mathbf{X}^T\mathbf{X}\mathbf{\beta}.\]
        关于\(\mathbf{\beta}\)求导得到
        \[\frac{\partial}{\partial \mathbf{\beta}}Q(\mathbf{\beta})=-2\mathbf{X}^T\mathbf{y}+2\mathbf{X}^T\mathbf{X}\mathbf{\beta}=0,\]
        就得到正规方程
        \[\mathbf{X}^T\mathbf{X}\mathbf{\beta}=\mathbf{X}^T\mathbf{y},\]
        由于假设\(\mathbf{X}\)列满秩，因此矩阵\(\mathbf{X}^T\mathbf{X}\)可逆，于是得到显式解
        \[\hat{\mathbf{\beta}}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}.\]
    \end{proof}
    因此OLS得到的预测
    \[\hat{\mathbf{y}}=\mathbf{X}\hat{\beta}=\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y},\]
    我们截取\(\mathbf{y}\)前面这部分矩阵，将其称为帽子矩阵\(\mathbf{H}\):
    \[\mathbf{H}=\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T.\]
    也就是\(\hat{\mathbf{y}}=\mathbf{H}\mathbf{y}\).从这里我们也可以看出\(\mathbf{y}\)的估计\(\hat{\mathbf{y}}\)也是\(\mathbf{y}\)的线性组合。
    \(\mathbf{H}\)有很多优良性质：
    \begin{property}
        \begin{enumerate}
            \item \(\mathbf{H}\)将\(\mathbf{y}\)投影到\(\mathbf{X}\)的列空间\(\mathcal{C}(\mathbf{X})\)上，从这个角度来看它是幂等的。当然也可以计算得到：\[\mathbf{H}^2=\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T=\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}(\mathbf{X}^T\mathbf{X})(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T=\mathbf{H}.\]
            \item \(\mathbf{H}\)对称。只要注意\[\mathbf{H}^T=(\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T)^T=\mathbf{H}.\]
            \item \(\mathbf{H}\)的迹(trace)\[tr(\mathbf{H})=p+1.\]只要注意到迹的性质：\(tr(AB)=tr(BA)\)再对\(\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\)应用即可，它的迹就是\(\mathbf{I_{p+1}}\)的迹。
        \end{enumerate}
    \end{property}
    真实值和OLS估计值之间的差异就是残差\(\mathbf{e}\)，残差可以看作是随机误差项的估计。
    \[\mathbf{e}=\mathbf{y}-\hat{\mathbf{y}}=\mathbf{y}-\mathbf{H}\mathbf{y}=(\mathbf{I}-\mathbf{H})\mathbf{y}.\]
    容易证明\(\mathbf{I-H}\)也是对称幂等的。

    我们用矩阵视角来看残差的数字性质。先看数学期望：残差的数学期望是0向量。
    
    代入\(\mathbf{Y}=\mathbf{X\beta+\epsilon}\),残差\(\mathbf{e}=\mathbf{(I-H)y}=(\mathbf{I-\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T})(\mathbf{X\beta+\epsilon})=\mathbf{(I-H)\epsilon}.\)由于经典假设下设计矩阵是固定的，不是随机矩阵，因此\(E[\mathbf{e}]=\mathbf{(I-H)}E[\mathbf{\epsilon}]=0.\)
    
    然后看残差的方差。先定义协方差矩阵。
    \begin{definition}[协方差矩阵]
        设有两列随机变量\(\mathbf{X}=\{X_n\},\mathbf{Y}=\{Y_m\}\),则它们的协方差矩阵\(\mathbf{cov(X,Y)}\)定义为\[\mathbf{cov(X,Y)}(ij)=cov(X_i,Y_j)=E[(\mathbf{X-E[X]})(\mathbf{Y-E[Y]})].\]
    \end{definition}
    \begin{property}
        \begin{enumerate}
            \item 和协方差同理，协方差矩阵也有双线性性：\[\mathbf{V[Ax+b]}=\mathbf{cov(AX+b,AX+b)}=\mathbf{Acov(X,X)A^T}.\]
            \item 协方差矩阵半正定。
        \end{enumerate}
    \end{property}
    残差\(\mathbf{e}\)的方差矩阵为
    \[\mathbf{cov(e,e)}=\mathbf{cov((I-H)y,(I-H)y)}=(\mathbf{(I-H)cov(y,y)(I-H)^T})=\sigma^2(\mathbf{I-H}).\]
    其中用到了\(\mathbf{cov(y,y)}=\sigma^2\mathbf{I_{p+1}}\).这样我们就知道\(\mathbf{e}\)第\(i\)个分量的方差
    \[V[e_i]=\sigma^2(1-h_{ii}).\]
    实际上这样就可以简便地计算一元线性回归中的残差的方差：
    设\[
    \mathbf{X}=
    \begin{bmatrix}
        1 & x_1\\
        1 & x_2\\
        \vdots\\
        1 & x_n    
    \end{bmatrix}
    \]
    则\(\mathbf{H}=\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\)就是
    $$ \mathbf{h_{ii}} = \frac{1}{n} + \frac{(x_i - \bar{x})^2}{L_{xx}} $$
    其中 $L_{xx} = \sum_{i=1}^n (x_i - \bar{x})^2$。

    因此\[V[e_i]=\sigma^2(1-h_{ii})=\sigma^2(1-\frac{1}{n} - \frac{(x_i - \bar{x})^2}{L_{xx}}).\]
    在一元的情形中我们说过，\(\sigma^2\)的无偏估计是\(\frac{1}{n-2}SSE\),现在我们在多元的情形下证明普遍情况。
    \begin{theorem}[方差的无偏估计]
        随机误差项的方差\(\sigma^2\)的一个无偏估计为
        \[\frac{1}{n-p-1}SSE.\]
    \end{theorem}
    \begin{proof}
        \[E[SSE]=\sum_{i=1}^{n}E[e_i^2]=\sum_{i=1}^{n}(V[e_i]+(E[e_i])^2)=\sum_{i=1}^{n}V[e_i]=\sum_{i=1}^{n}\sigma^2(1-h_{ii})=(n-p-1)\sigma^2.\]
        其中用了\(E[e_i]=0,tr(\mathbf{H})=p+1.\)
    \end{proof}
    \subsection{多元线性回归参数的极大似然估计}
    类比一元情况，当假设随机误差项服从\(N(\mathbf{0},\sigma^2\mathbf{I_n})\)时\(y\)也服从正态分布\(\mathbf{X\beta},\sigma^2\mathbf{I_n}\).则极大似然函数为
    \[L(\beta,\sigma^2)=(2\pi\sigma^2)^{-\frac{n}{2}}\exp{(-\frac{1}{2\sigma^2}||\mathbf{y-X\beta}||_2^2)},\]
    同样的步骤可以得出\(\beta\)的MLE同OLS给出的形式，而\(\sigma^2\)的MLE为\(\frac{1}{n}SSE\).这个估计是渐近无偏的。
    \subsection{回归系数的性质}
    在上面我们已经说过，\(\hat{\beta}\)是\(\mathbf{y}\)的线性变换，因此OLS得到的回归系数也服从正态分布。下面来看回归系数的性质。
    \begin{property}
        \begin{enumerate}
        \item \(\hat{\beta}\)是\(\beta\)的无偏估计。
        \item \(\hat{\beta}\)的协方差矩阵为\(\sigma^2(\mathbf{X}^T\mathbf{X})^{-1}\).
        \item 在Gauss-Markov条件下，任给向量\(\mathbf{c}\in\mathbb{R}^{p+1}\)，线性函数\(\mathbf{c}^T\beta\)的最小方差线性无偏估计是\(\mathbf{c}^T\hat{\beta}\).
        \item 若假设随机误差项服从正态分布，则\(\hat{\beta},\hat{\sigma}^2\)相互独立。并且\(\hat{\beta}\sim N(\beta,\sigma^2(\mathbf{(X^TX)^{-1}})),SSE/\sigma^2\sim \chi_{n-p-1}^2.\)
    \end{enumerate}
    \end{property}
    \begin{proof}
        \begin{enumerate}
            \item 经典假设下设计矩阵非随机。故\(E[\hat{\beta}]=E[(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}]=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^TE[\mathbf{y}]=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{X}\beta=\beta.\)
            \item 应用协方差矩阵的性质得到\[V[\hat{\beta}]=V[(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}]=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^TV[\mathbf{y}]\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\sigma^2\mathbf{I_n}\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}=\sigma^2(\mathbf{X}^T\mathbf{X})^{-1}.\]
            \item 设线性函数\(\mathbf{c}^T\beta\)的一个无偏估计为\(\mathbf{d}^Ty\),即\(E[\mathbf{d}^Ty]=E[\mathbf{d}^T\mathbf{X}\beta]=\mathbf{d}^T\mathbf{X}\beta=\mathbf{c}^T\beta.\)这个式子对于任意的\(\beta\in \mathbb{R}^{p+1}\)都成立，因此由任意性知道\(\mathbf{d}^T\mathbf{X}=\mathbf{c}^T\)成立。要证明结论成立，只要证明\[V[\mathbf{d}^Ty]=\sigma^2\mathbf{d}^T\mathbf{d}\ge V[\mathbf{c}^T\hat{\beta}]=\sigma^2\mathbf{c}^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{c},\]即证明\[\mathbf{d}^T\mathbf{d}\ge \mathbf{c}^T(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{c}.\]代入\(\mathbf{d}^T\mathbf{X}=\mathbf{c}^T\)化简得到\[\mathbf{d}^T(\mathbf{I-H})\mathbf{d}\ge 0.\]也就是说只要证明\(\mathbf{I-H}\)是半正定的。实际上我们知道\(\mathbf{I-H}\)就是一个将\(\mathbf{y}\)投影到\(\mathbf{X}\)的列空间\(\mathcal{C}(\mathbf{X})\)上的投影矩阵，它是对称幂等矩阵，因此它是半正定的。
        \end{enumerate}
    \end{proof}


    \chapter{时间序列分析}

    \chapter{统计模拟}

    \chapter{抽样方法}



\end{document}